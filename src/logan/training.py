import time
from collections import defaultdict
from typing import Any, Callable, Optional, Sequence

import jax
import jax.numpy as jnp
import numpy as np
import optax
from ase import Atoms
from numpy.typing import ArrayLike

from .models import CriticModel, GeneratorModel
from .utilities import *


def make_atomic_number_map(atoms_arr: Sequence[Atoms]):
    """Creates a mapping between atomic numbers and internal type indices.

    Args:
        atoms_arr: The sequence of Atoms objects used to create the map.

    Returns:
        A dictionary mapping the atomic numbers to internal type indices.
    """
    atomtypes = set()
    for atoms in atoms_arr:
        for anum in atoms.get_atomic_numbers():
            atomtypes.add(anum)

    result = {}
    for i, anum in enumerate(sorted(atomtypes)):
        result[anum] = i

    return result


def make_training_dict(
    atoms_arr: Sequence[Atoms],
    descriptor_method: Callable,
    anum_map: dict[int, int],
    n_samples=0,
):
    """Generates a dictionary holding possible "real descriptors" indexed by
    the atom types

    Args:
        atoms_arr: The sequence of Atoms objects to take descriptors from.
        descriptor_method: The method used to generate descriptors.
        anum_map: Mapping between atomic numbers and internal type indices.

    Returns:
        A dictionary of the list of possible "real descriptors" indexed by
        internal types.
    """
    result = defaultdict(list)

    for atoms in atoms_arr:
        pos = atoms.positions
        cell = atoms.cell[...]
        type = np.array(
            [anum_map[anum] for anum in atoms.get_atomic_numbers()]
        )
        descriptors = descriptor_method(pos, type, pos, type, cell)
        descriptors = descriptors.reshape((descriptors.shape[0], -1))

        if n_samples == 0:
            for t, desc in zip(type, descriptors):
                result[t].append(desc)
        else:
            for t in np.unique(type):
                indices = np.where(type == t)[0]
                chosen = np.random.choice(indices, n_samples)

                for ind in chosen:
                    result[t].append(descriptors[ind])

    for t in result:
        result[t] = np.array(result[t])

    return result


def make_real_sampler(
    types: Sequence[int], n_batch: int, training_dict: dict[int, list]
):
    """Creates a sampler method which generates "real descriptors".

    Args:
        types: The internal types of the atoms for which "real descriptors" are
            expected.
        n_batch: The number of batches to generate the descriptors for.
        training_dict: The dictionary of descriptors generated by "make_training_dict".

    Returns:
        A sampler method which uses a PRNG key to generate "real descriptors"
            in the shape of (n_batch, len(types), len(descriptor)).
    """
    expected_shape = (n_batch, len(types), training_dict[0][0].shape[-1])
    type_positions = {}

    for t in types:
        type_positions[t] = np.where(types == t)[0]

    def sampler(rng):
        result = jnp.zeros(shape=expected_shape)

        for t in type_positions:
            rng, key = jax.random.split(rng)
            indices = type_positions[t]
            result = result.at[:, indices, :].set(
                jax.random.choice(
                    key, training_dict[t], (n_batch, len(indices))
                )
            )

        return result, rng

    return sampler


def create_critic_step(
    critic: CriticModel,
    generator: GeneratorModel,
    descriptor_method: Callable,
    postprocess: Callable,
    n_batch: int,
    optimizer_crit: optax.GradientTransformation,
):
    """Creates the critic training step.

    Args:
        critic: The CriticModel to use.
        generator: The GeneratorModel to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        n_batch: Number of batches in the training.
        optimizer_crit: The Optax optimizer object for critic training.

    Returns:
        A method which does one step in the critic training. It uses the
        current critic weights, the current generator weights, the critic
        optimizer state, a batch of "real" descriptors and a PRNG key
        respectively. It returns with the updated critic weights, critic
        optimizer state and PRNG key.
    """
    generate_batch_descriptor = create_generate_batch_descriptor(
        descriptor_method
    )
    generate_batch = jax.vmap(generator.apply, (None, 0), 0)
    criticize_batch = jax.vmap(critic.apply, (None, 0), 0)
    postprocess_batch = jax.vmap(postprocess)

    @jax.jit
    def critic_step(params_crit, params_gen, opt_state_crit, real_desc, rng):
        rng, key = jax.random.split(rng)
        fake_latent = jax.random.normal(key, (n_batch, generator.n_latent))
        intermediate = generate_batch(params_gen, fake_latent)
        (
            all_fake_pos,
            all_fake_type,
            fake_pos,
            fake_type,
            cell,
        ) = postprocess_batch(intermediate)

        fake_desc = generate_batch_descriptor(
            all_fake_pos, all_fake_type, fake_pos, fake_type, cell
        )
        fake_desc = jnp.reshape(fake_desc, (-1, fake_desc.shape[-1]))
        real_desc = jnp.reshape(real_desc, (-1, real_desc.shape[-1]))

        def loss(params):
            fake_preds = criticize_batch(params, fake_desc)
            real_preds = criticize_batch(params, real_desc)

            return jnp.mean(real_preds - fake_preds)

        grads = jax.grad(loss)(params_crit)
        updates, opt_state_crit = optimizer_crit.update(
            grads, opt_state_crit, params_crit
        )
        params_crit = optax.apply_updates(params_crit, updates)

        return params_crit, opt_state_crit, rng

    return critic_step


def create_gradient_penalty_step(
    critic: CriticModel,
    generator: GeneratorModel,
    descriptor_method: Callable,
    postprocess: Callable,
    n_batch: int,
    optimizer_crit: optax.GradientTransformation,
):
    """Creates the gradient-penalty step for the critic training.

    Args:
        critic: The CriticModel to use.
        generator: The GeneratorModel to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        n_batch: Number of batches in the training.
        optimizer_crit: The Optax optimizer object for critic training.

    Returns:
        A method which does one step of enforcing the gradient-penalty of the
        critic. It uses the current critic weights, the current generator
        weights, the critic optimizer state, a batch of "real" descriptors and
        a PRNG key respectively. It returns with the updated critic weights,
        critic optimizer state and PRNG key.
    """
    generate_batch_descriptor = create_generate_batch_descriptor(
        descriptor_method
    )
    generate_batch = jax.vmap(generator.apply, (None, 0), 0)
    postprocess_batch = jax.vmap(postprocess)

    @jax.jit
    def critic_gp_step(
        params_crit, params_gen, opt_state_crit, real_desc, rng
    ):
        rng, key = jax.random.split(rng)
        fake_latent = jax.random.normal(key, (n_batch, generator.n_latent))
        intermediate = generate_batch(params_gen, fake_latent)
        all_fake_pos, all_fake_type, fake_pos, fake_type, cell = (
            postprocess_batch(intermediate)
        )

        fake_desc = generate_batch_descriptor(
            all_fake_pos, all_fake_type, fake_pos, fake_type, cell
        )
        fake_desc = jnp.reshape(fake_desc, (-1, fake_desc.shape[-1]))

        rng, key = jax.random.split(rng)
        weights = jax.random.uniform(key, shape=(fake_desc.shape[0], 1))
        mid_descriptors = real_desc * weights + fake_desc * (1 - weights)

        def loss(params):
            def criticize(descriptor):
                return critic.apply(params, descriptor)

            critic_grad = jax.grad(criticize)
            critic_grad_batch = jax.vmap(critic_grad)
            grad_norms = jnp.linalg.norm(
                critic_grad_batch(mid_descriptors), axis=1
            )

            return jnp.mean((grad_norms - 1) ** 2)

        grads = jax.grad(loss)(params_crit)
        updates, opt_state_crit = optimizer_crit.update(grads, opt_state_crit)
        params_crit = optax.apply_updates(params_crit, updates)

        return params_crit, opt_state_crit, rng

    return critic_gp_step


def create_generator_step(
    critic: CriticModel,
    generator: GeneratorModel,
    descriptor_method: Callable,
    postprocess: Callable,
    n_batch: int,
    optimizer_gen: optax.GradientTransformation,
):
    """Creates the generator training step.

    Args:
        critic: The CriticModel to use.
        generator: The GeneratorModel to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        n_batch: Number of batches in the training.
        optimizer_gen: The Optax optimizer object for generator training.

    Returns:
        A method which does one step of the generator training. It uses the
        current critic weights, the current generator weights, the generator
        optimizer state and a PRNG key respectively.
        It returns with the updated generator weights, generator optimizer
        state and PRNG key.
    """
    generate_batch_descriptor = create_generate_batch_descriptor(
        descriptor_method
    )
    generate_batch = jax.vmap(generator.apply, (None, 0), 0)
    criticize_batch = jax.vmap(critic.apply, (None, 0), 0)
    postprocess_batch = jax.vmap(postprocess)

    @jax.jit
    def generator_step(params_crit, params_gen, opt_state_gen, rng):
        rng, key = jax.random.split(rng)
        fake_latent = jax.random.normal(key, (n_batch, generator.n_latent))

        def loss(params):
            intermediate = generate_batch(params, fake_latent)
            all_fake_pos, all_fake_type, fake_pos, fake_type, cell = (
                postprocess_batch(intermediate)
            )

            fake_desc = generate_batch_descriptor(
                all_fake_pos, all_fake_type, fake_pos, fake_type, cell
            )
            fake_desc = jnp.reshape(fake_desc, (-1, fake_desc.shape[-1]))
            fake_preds = criticize_batch(params_crit, fake_desc)
            return jnp.mean(fake_preds)

        grads = jax.grad(loss)(params_gen)
        updates, opt_state_gen = optimizer_gen.update(grads, opt_state_gen)
        params_gen = optax.apply_updates(params_gen, updates)

        return params_gen, opt_state_gen, rng

    return generator_step


def create_full_training_step(
    critic: CriticModel,
    generator: GeneratorModel,
    descriptor_method: Callable,
    postprocess: Callable,
    types: Sequence[int],
    training_dict: dict[int, list],
    optimizer_crit: optax.GradientTransformation,
    optimizer_gen: optax.GradientTransformation,
    n_batch: int,
    n_crit_per_gen: int = 5,
    n_gp_per_crit: int = 2,
):
    """Creates a full WGAN training step.

    Args:
        critic: The CriticModel to use.
        generator: The GeneratorModel to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        types: The internal types of the atoms for which "real descriptors" are
            expected.
        training_dict: The dictionary of descriptors generated by "make_training_dict".
        optimizer_crit: The Optax optimizer object for critic training.
        optimizer_gen: The Optax optimizer object for generator training.
        n_batch: Number of batches in the training.
        n_crit_per_gen: Number of critic training steps per generator steps. Default: 5
        n_gp_pert_crit: Number of gradient penalty steps per critic steps. Default: 2


    Returns:
        A method which does one full training step. It uses the
        current generator weights, the generator optimizer state,
        critic weights, critic optimizer state and a PRNG key respectively.
        It returns with the updated generator weights, generator optimizer
        state, critic weights, critic optimizer state and the next PRNG key.
    """

    sampler = make_real_sampler(types, n_batch, training_dict)

    train_step_gen = create_generator_step(
        critic=critic,
        generator=generator,
        descriptor_method=descriptor_method,
        postprocess=postprocess,
        n_batch=n_batch,
        optimizer_gen=optimizer_gen,
    )

    train_step_crit = create_critic_step(
        critic=critic,
        generator=generator,
        descriptor_method=descriptor_method,
        postprocess=postprocess,
        n_batch=n_batch,
        optimizer_crit=optimizer_crit,
    )

    train_step_gp = create_gradient_penalty_step(
        critic=critic,
        generator=generator,
        descriptor_method=descriptor_method,
        postprocess=postprocess,
        n_batch=n_batch,
        optimizer_crit=optimizer_crit,
    )

    n_points = len(types)

    def train_step(
        params_gen, opt_state_gen, params_crit, opt_state_crit, rng
    ):
        params_gen, opt_state_gen, rng = train_step_gen(
            params_crit, params_gen, opt_state_gen, rng
        )
        for _ in range(n_crit_per_gen):
            real_desc, rng = sampler(rng)
            real_desc = real_desc.reshape((n_batch * n_points, -1))
            params_crit, opt_state_crit, rng = train_step_crit(
                params_crit, params_gen, opt_state_crit, real_desc, rng
            )
            for _ in range(n_gp_per_crit):
                real_desc, rng = sampler(rng)
                real_desc = real_desc.reshape((n_batch * n_points, -1))
                params_crit, opt_state_crit, rng = train_step_gp(
                    params_crit, params_gen, opt_state_crit, real_desc, rng
                )

        return params_gen, opt_state_gen, params_crit, opt_state_crit, rng

    return train_step


def train(
    training_dict: dict[int, list],
    desc_generator_method: Callable,
    types: Sequence[int],
    gen_cell: ArrayLike,
    n_points: int,
    n_dimensions: int = 3,
    n_latent: int = 20,
    n_scalars: int = 0,
    generator_shape: Sequence[int] = [512, 256, 128, 64, 32],
    critic_shape: Sequence[int] = [256, 128, 64, 32],
    n_epochs: int = 60000,
    n_step_per_snapshot: int = 6000,
    n_step_per_validate: int = 100,
    n_validate_batch: int = 500,
    seed: int = 0,
    n_batch: int = 15,
    n_crit: int = 5,
    n_gp_per_crit: int = 2,
    lr_crit: float = 10**-4,
    lr_gen: float = 10**-4,
    periodic: bool = False,
    mult: ArrayLike = np.array([1, 1, 1]),
):
    """Trains a BesselGAN from scratch

    Args:
        training_dict: The dictionary of descriptors generated by "make_training_dict".
        desc_generator_method: The method used for descriptor generation.
        types: Sequence of internal types of the generated atoms.
        gen_cell: Numpy array of unit cell
        n_points: The number of atoms to predict coordinates for.
        n_dimensions: Dimensionality (usually three-dimensional data)
        n_latent: Number of latent variables (i.e. random numbers) that serve as input to the generator
        n_scalars: Number of extra scalars to predict for postprocessing
        generator_shape: The sequence of hidden layer widths
        critic_shape: The sequence of hidden layer widths
        n_epochs: The number of training epochs
        n_step_per_snapshot: Frequency of saving snapshots of the generator
        n_step_per_validate: Frequency of computing and printing validation statistics
        n_validate_batch: Batch size for validation
        seed: Seed for initializing the model
        n_batch: Batch size for training
        n_crit: Number of critic training steps per generator steps
        n_gp_per_crit: Number of gradient penalty steps per critic steps
        lr_crit: Learning rate of the critic
        lr_gen: Learning rate of the generator
        periodic: Whether to use periodic boundary conditions to create a supercell during pre- and postprocessing
        mult: Supercell size

    Returns:
        The trained generator
    """

    print("Start training...")

    rng = jax.random.PRNGKey(seed)

    # Postprocessing function
    def postprocess(intermediate):
        # Currently only implemented for 3 dimensions in the periodic case!
        pos = jnp.reshape(intermediate, (-1, 3))
        pos = jnp.matmul(pos, gen_cell)

        if periodic:
            assert n_dimensions == 3
            res_pos = jnp.zeros((0, n_dimensions))
            res_types = jnp.zeros((0))

            for i in range(mult[0]):
                for j in range(mult[1]):
                    for k in range(mult[2]):
                        res_pos = jnp.concatenate(
                            [
                                res_pos,
                                pos
                                + i * gen_cell[0]
                                + j * gen_cell[1]
                                + k * gen_cell[2],
                            ]
                        )
                        res_types = jnp.concatenate([res_types, types])
            res_cell = (gen_cell[:, :].T * mult).T
            return res_pos, res_types, pos, types, res_cell

        return pos, types, pos, types, jnp.zeros(shape=(3, 3))

    generator = GeneratorModel(
        features=generator_shape,
        n_points=n_points,
        n_dimensions=n_dimensions,
        n_latent=n_latent,
        n_scalars=n_scalars,
    )

    critic = CriticModel(features=critic_shape)

    optimizer_gen = optax.rmsprop(lr_gen)
    optimizer_crit = optax.rmsprop(lr_crit)

    rng, key = jax.random.split(rng)
    params_gen = generator.init(key, jax.random.normal(key, shape=(n_latent,)))
    rng, key = jax.random.split(rng)
    params_crit = critic.init(key, training_dict[0][0])

    opt_state_gen = optimizer_gen.init(params_gen)
    opt_state_crit = optimizer_crit.init(params_crit)

    train_step = jax.jit(
        create_full_training_step(
            critic,
            generator,
            desc_generator_method,
            postprocess,
            types,
            training_dict,
            optimizer_crit,
            optimizer_gen,
            n_batch,
            n_crit,
            n_gp_per_crit,
        )
    )

    eval_single_desc = create_evaluate_single_descriptor(critic)
    eval_batch_desc = create_evaluate_batch_descriptor(critic)

    generate_single_desc = create_generate_descriptor(desc_generator_method)
    generate_batch_desc = create_generate_batch_descriptor(
        desc_generator_method
    )

    generate_structures = create_generate_structures(
        generator, postprocess, n_latent
    )

    sampler = make_real_sampler(types, n_batch, training_dict)
    sampler_validate = make_real_sampler(
        types, n_validate_batch, training_dict
    )

    real_preds = []
    fake_preds = []
    gen_snapshots = []
    crit_snapshots = []

    start = time.time()
    for i in range(n_epochs):
        if i % n_step_per_validate == 0:
            rng, key = jax.random.split(rng)
            allpos, alltype, curr_pos, curr_types, cell = generate_structures(
                params_gen, key, n_batch
            )
            desc = generate_batch_desc(
                allpos, alltype, curr_pos, curr_types, cell
            )
            desc = desc.reshape((-1, desc.shape[-1]))

            pred_fake = np.mean(eval_batch_desc(params_crit, desc))
            real_desc, rng = sampler(rng)
            real_desc = real_desc.reshape((n_batch * n_points, -1))
            pred_real = np.mean(eval_batch_desc(params_crit, real_desc))

            real_preds.append(pred_real)
            fake_preds.append(pred_fake)

            print(
                i,
                pred_fake,
                pred_real,
                pred_fake - pred_real,
                time.time() - start,
            )
            start = time.time()
        if i % n_step_per_snapshot == 0:
            gen_snapshots.append(params_gen)
            crit_snapshots.append(params_crit)

        params_gen, opt_state_gen, params_crit, opt_state_crit, rng = (
            train_step(
                params_gen, opt_state_gen, params_crit, opt_state_crit, rng
            )
        )

    return gen_snapshots[-1]


def generate_structs(
    n: int,
    params_gen: dict,
    generator_shape: Sequence[int],
    n_latent: int,
    n_scalars: int,
    n_dimensions: int,
    reverse_anum_map: dict,
    gen_cell: ArrayLike,
    n_points: int,
    periodic: bool,
    types: Sequence[int],
    seed: int = 0,
):
    """Generates structures with a trained BesselGAN model

    Args:
        n: Number of structures to generate
        params_gen: Dictionary of generator parameters
        generator_shape: The sequence of hidden layer widths
        n_latent: Number of latent variables (i.e. random numbers) that serve as input to the generator
        n_scalars: Number of extra scalars to predict for postprocessing
        n_dimensions: Dimensionality (usually three-dimensional data)
        reverse_anum_map: Dictionary to map types back to atomic numbers
        gen_cell: Numpy array of unit cell
        n_points: The number of atoms to predict coordinates for.
        periodic: Whether to use periodic boundary conditions to create a supercell during pre- and postprocessing
        types: Sequence of internal types of the generated atoms.
        seed: Seed for initializing the model

    Returns:
        A list of generated structures
    """

    # Postprocessing function
    def postprocess(intermediate):
        pos = jnp.reshape(intermediate, (-1, 3))
        pos = jnp.matmul(pos, gen_cell)

        if periodic:
            return pos, types, pos, types, gen_cell

        return pos, types, pos, types, jnp.zeros(shape=(3, 3))

    rng = jax.random.PRNGKey(seed)
    rng, key = jax.random.split(rng)

    generator = GeneratorModel(
        features=generator_shape,
        n_points=n_points,
        n_dimensions=n_dimensions,
        n_latent=n_latent,
        n_scalars=n_scalars,
    )

    generate_structures = create_generate_structures(
        generator, postprocess, n_latent
    )
    all_pos, all_type, _, _, all_cell = generate_structures(params_gen, key, n)
    gen_atoms_list = []
    for i in range(n):
        atom = Atoms(positions=all_pos[i])
        atom.set_atomic_numbers(
            np.array([reverse_anum_map[int(j)] for j in all_type[i]]).flatten()
        )
        if periodic:
            atom.cell = all_cell[i]
            atom.pbc = True
        #        else:
        #            atom.center(vacuum=3.0)
        gen_atoms_list.append(atom)

    return gen_atoms_list
